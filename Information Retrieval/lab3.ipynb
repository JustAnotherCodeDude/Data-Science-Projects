{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whoosh in c:\\users\\natha\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh import index, writing\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import *\n",
    "from whoosh.qparser import QueryParser\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"lab-data\"\n",
    "DOCUMENTS_DIR = os.path.join(DATA_DIR, \"documents\")\n",
    "TOPIC_FILE = os.path.join(DATA_DIR, \"air.topics\")\n",
    "QRELS_FILE = os.path.join(DATA_DIR, \"air.qrels\")\n",
    "\n",
    "# For windows:\n",
    "TREC_EVAL = os.path.join(\"trec_eval\", \"trec_eval.exe\")\n",
    "\n",
    "#For mac:\n",
    "#TREC_EVAL = os.path.join(\"trec_eval\", \"trec_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createIndex(schema):\n",
    "    # Generate a temporary directory for the index\n",
    "    indexDir = tempfile.mkdtemp()\n",
    "\n",
    "    # create and return the index\n",
    "    return index.create_in(indexDir, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, define a Schema for the index\n",
    "mySchema = Schema(file_path = ID(stored=True),\n",
    "                  file_content = TEXT(analyzer = RegexTokenizer()))\n",
    "\n",
    "# now, create the index at the path INDEX_DIR based on the new schema\n",
    "myIndex = createIndex(mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addFilesToIndex(indexObj, fileList):\n",
    "    # open writer\n",
    "    writer = writing.BufferedWriter(indexObj, period=None, limit=1000)\n",
    "\n",
    "    try:\n",
    "        # write each file to index\n",
    "        for docNum, filePath in enumerate(fileList):\n",
    "            with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "                fileContent = f.read()\n",
    "                writer.add_document(file_path = filePath,\n",
    "                                    file_content = fileContent)\n",
    "\n",
    "                # print status every 1000 documents\n",
    "                if ((docNum+1) % 5 == 0):\n",
    "                    print(\"already indexed:\", docNum+1)\n",
    "        print(\"done indexing.\")\n",
    "\n",
    "    finally:\n",
    "        # close the index\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a list of files to index\n",
    "filesToIndex = [str(filePath) for filePath in Path(DOCUMENTS_DIR).glob(\"**/*\") if filePath.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lab-data\\\\documents\\\\email01',\n",
       " 'lab-data\\\\documents\\\\email02',\n",
       " 'lab-data\\\\documents\\\\email03',\n",
       " 'lab-data\\\\documents\\\\email04',\n",
       " 'lab-data\\\\documents\\\\email05']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the list\n",
    "filesToIndex[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files: 11\n"
     ]
    }
   ],
   "source": [
    "# count files to index\n",
    "print(\"number of files:\", len(filesToIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already indexed: 5\n",
      "already indexed: 10\n",
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "addFilesToIndex(myIndex, filesToIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser = QueryParser(\"file_content\", schema=myIndex.schema)\n",
    "mySearcher = myIndex.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email01 0 2.6746417187049216\n"
     ]
    }
   ],
   "source": [
    "# run a sample query for the phrase \"item\"\n",
    "sampleQuery = myQueryParser.parse(\"item\")\n",
    "sampleQueryResults = mySearcher.search(sampleQuery, limit=None)\n",
    "\n",
    "# inspect the result:\n",
    "# for each document print the rank and the score\n",
    "for (docnum, result) in enumerate(sampleQueryResults):\n",
    "    score = sampleQueryResults.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation using TREC_EVAL\n",
    "In order to evaluate our results we will use a topic file - a list of topics we use to evaluate our IR system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 ducks\n",
      "02 ig nobel prizes\n",
      "03 mathematics\n",
      "04 flowing hair\n",
      "05 music\n",
      "06 AIR TV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the topic file\n",
    "with open(TOPIC_FILE, \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare our evaluate our results with a set of judged results(qrels file) using TREC_EVAL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 0 email01 0\n",
      "01 0 email02 0\n",
      "01 0 email03 0\n",
      "01 0 email04 1\n",
      "01 0 email05 1\n",
      "01 0 email06 1\n",
      "01 0 email07 0\n",
      "01 0 email08 0\n",
      "01 0 email09 0\n",
      "01 0 email10 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the first 10 lines in the qrels file\n",
    "with open(QRELS_FILE, \"r\") as f:\n",
    "    qrels10 = f.readlines()[:10]\n",
    "    print(\"\".join(qrels10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follwing function takes a topic file, a qrels file, a query parser and a searcher and use TREC_EVAL to compare our results with the provided qrels file (see assignment PDF for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trecEval(topicFile, qrelsFile, queryParser, searcher):\n",
    "    # Load topic file - a list of topics(search phrases) used for evalutation\n",
    "    with open(topicFile, \"r\") as tf:\n",
    "        topics = tf.read().splitlines()\n",
    "\n",
    "    # create an output file to which we'll write our results\n",
    "    tempOutputFile = tempfile.mkstemp()[1]\n",
    "    with open(tempOutputFile, \"w\") as outputTRECFile:\n",
    "        # for each evaluated topic:\n",
    "        # build a query and record the results in the file in TREC_EVAL format\n",
    "        for topic in topics:\n",
    "            topic_id, topic_phrase = tuple(topic.split(\" \", 1))\n",
    "            topicQuery = queryParser.parse(topic_phrase)\n",
    "            topicResults = searcher.search(topicQuery, limit=None)\n",
    "            for (docnum, result) in enumerate(topicResults):\n",
    "                score = topicResults.score(docnum)\n",
    "                outputTRECFile.write(\"%s Q0 %s %d %lf test\\n\" % (topic_id, os.path.basename(result[\"file_path\"]), docnum, score))\n",
    "    \n",
    "    result = subprocess.run([TREC_EVAL,'-q', qrelsFile, tempOutputFile], stdout=subprocess.PIPE)\n",
    "    print(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ret               \t01\t1\n",
      "num_rel               \t01\t3\n",
      "num_rel_ret           \t01\t1\n",
      "map                   \t01\t0.3333\n",
      "Rprec                 \t01\t0.3333\n",
      "bpref                 \t01\t0.3333\n",
      "recip_rank            \t01\t1.0000\n",
      "iprec_at_recall_0.00  \t01\t1.0000\n",
      "iprec_at_recall_0.10  \t01\t1.0000\n",
      "iprec_at_recall_0.20  \t01\t1.0000\n",
      "iprec_at_recall_0.30  \t01\t1.0000\n",
      "iprec_at_recall_0.40  \t01\t0.0000\n",
      "iprec_at_recall_0.50  \t01\t0.0000\n",
      "iprec_at_recall_0.60  \t01\t0.0000\n",
      "iprec_at_recall_0.70  \t01\t0.0000\n",
      "iprec_at_recall_0.80  \t01\t0.0000\n",
      "iprec_at_recall_0.90  \t01\t0.0000\n",
      "iprec_at_recall_1.00  \t01\t0.0000\n",
      "P_5                   \t01\t0.2000\n",
      "P_10                  \t01\t0.1000\n",
      "P_15                  \t01\t0.0667\n",
      "P_20                  \t01\t0.0500\n",
      "P_30                  \t01\t0.0333\n",
      "P_100                 \t01\t0.0100\n",
      "P_200                 \t01\t0.0050\n",
      "P_500                 \t01\t0.0020\n",
      "P_1000                \t01\t0.0010\n",
      "num_ret               \t05\t1\n",
      "num_rel               \t05\t2\n",
      "num_rel_ret           \t05\t0\n",
      "map                   \t05\t0.0000\n",
      "Rprec                 \t05\t0.0000\n",
      "bpref                 \t05\t0.0000\n",
      "recip_rank            \t05\t0.0000\n",
      "iprec_at_recall_0.00  \t05\t0.0000\n",
      "iprec_at_recall_0.10  \t05\t0.0000\n",
      "iprec_at_recall_0.20  \t05\t0.0000\n",
      "iprec_at_recall_0.30  \t05\t0.0000\n",
      "iprec_at_recall_0.40  \t05\t0.0000\n",
      "iprec_at_recall_0.50  \t05\t0.0000\n",
      "iprec_at_recall_0.60  \t05\t0.0000\n",
      "iprec_at_recall_0.70  \t05\t0.0000\n",
      "iprec_at_recall_0.80  \t05\t0.0000\n",
      "iprec_at_recall_0.90  \t05\t0.0000\n",
      "iprec_at_recall_1.00  \t05\t0.0000\n",
      "P_5                   \t05\t0.0000\n",
      "P_10                  \t05\t0.0000\n",
      "P_15                  \t05\t0.0000\n",
      "P_20                  \t05\t0.0000\n",
      "P_30                  \t05\t0.0000\n",
      "P_100                 \t05\t0.0000\n",
      "P_200                 \t05\t0.0000\n",
      "P_500                 \t05\t0.0000\n",
      "P_1000                \t05\t0.0000\n",
      "num_ret               \t06\t3\n",
      "num_rel               \t06\t2\n",
      "num_rel_ret           \t06\t2\n",
      "map                   \t06\t0.8333\n",
      "Rprec                 \t06\t0.5000\n",
      "bpref                 \t06\t1.0000\n",
      "recip_rank            \t06\t1.0000\n",
      "iprec_at_recall_0.00  \t06\t1.0000\n",
      "iprec_at_recall_0.10  \t06\t1.0000\n",
      "iprec_at_recall_0.20  \t06\t1.0000\n",
      "iprec_at_recall_0.30  \t06\t1.0000\n",
      "iprec_at_recall_0.40  \t06\t1.0000\n",
      "iprec_at_recall_0.50  \t06\t1.0000\n",
      "iprec_at_recall_0.60  \t06\t0.6667\n",
      "iprec_at_recall_0.70  \t06\t0.6667\n",
      "iprec_at_recall_0.80  \t06\t0.6667\n",
      "iprec_at_recall_0.90  \t06\t0.6667\n",
      "iprec_at_recall_1.00  \t06\t0.6667\n",
      "P_5                   \t06\t0.4000\n",
      "P_10                  \t06\t0.2000\n",
      "P_15                  \t06\t0.1333\n",
      "P_20                  \t06\t0.1000\n",
      "P_30                  \t06\t0.0667\n",
      "P_100                 \t06\t0.0200\n",
      "P_200                 \t06\t0.0100\n",
      "P_500                 \t06\t0.0040\n",
      "P_1000                \t06\t0.0020\n",
      "runid                 \tall\ttest\n",
      "num_q                 \tall\t3\n",
      "num_ret               \tall\t5\n",
      "num_rel               \tall\t7\n",
      "num_rel_ret           \tall\t3\n",
      "map                   \tall\t0.3889\n",
      "gm_map                \tall\t0.0141\n",
      "Rprec                 \tall\t0.2778\n",
      "bpref                 \tall\t0.4444\n",
      "recip_rank            \tall\t0.6667\n",
      "iprec_at_recall_0.00  \tall\t0.6667\n",
      "iprec_at_recall_0.10  \tall\t0.6667\n",
      "iprec_at_recall_0.20  \tall\t0.6667\n",
      "iprec_at_recall_0.30  \tall\t0.6667\n",
      "iprec_at_recall_0.40  \tall\t0.3333\n",
      "iprec_at_recall_0.50  \tall\t0.3333\n",
      "iprec_at_recall_0.60  \tall\t0.2222\n",
      "iprec_at_recall_0.70  \tall\t0.2222\n",
      "iprec_at_recall_0.80  \tall\t0.2222\n",
      "iprec_at_recall_0.90  \tall\t0.2222\n",
      "iprec_at_recall_1.00  \tall\t0.2222\n",
      "P_5                   \tall\t0.2000\n",
      "P_10                  \tall\t0.1000\n",
      "P_15                  \tall\t0.0667\n",
      "P_20                  \tall\t0.0500\n",
      "P_30                  \tall\t0.0333\n",
      "P_100                 \tall\t0.0100\n",
      "P_200                 \tall\t0.0050\n",
      "P_500                 \tall\t0.0020\n",
      "P_1000                \tall\t0.0010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trecEval(TOPIC_FILE, QRELS_FILE, myQueryParser, mySearcher) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Evaluating different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting our index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is empty? False\n",
      "Number of indexed files: 11\n"
     ]
    }
   ],
   "source": [
    "# Is it empty?\n",
    "print(\"Index is empty?\", myIndex.is_empty())\n",
    "\n",
    "# How many files indexed?\n",
    "print(\"Number of indexed files:\", myIndex.doc_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a reader object on the index\n",
    "myReader = myIndex.reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, {'file_path': 'lab-data\\\\documents\\\\email01'}),\n",
       " (1, {'file_path': 'lab-data\\\\documents\\\\email02'}),\n",
       " (2, {'file_path': 'lab-data\\\\documents\\\\email03'}),\n",
       " (3, {'file_path': 'lab-data\\\\documents\\\\email04'}),\n",
       " (4, {'file_path': 'lab-data\\\\documents\\\\email05'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 5 indexed documents\n",
    "[(docnum, doc_dict) for (docnum, doc_dict) in myReader.iter_docs()][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Care',\n",
       " 'Carlos',\n",
       " 'Carmen',\n",
       " 'Carnivalesque',\n",
       " 'Carolina',\n",
       " 'Case',\n",
       " 'Cat',\n",
       " 'Catalysis',\n",
       " 'Catalyst',\n",
       " 'Catchers',\n",
       " 'Cater',\n",
       " 'Caused',\n",
       " 'Caveat',\n",
       " 'CbZF1d0021swQuc57kfqHt',\n",
       " 'Cechetto',\n",
       " 'Ceder',\n",
       " 'Celebratory',\n",
       " 'Center',\n",
       " 'Cereal',\n",
       " 'Ceremony',\n",
       " 'Cerrahi',\n",
       " 'Certolizumab',\n",
       " 'Cervical',\n",
       " 'Chair',\n",
       " 'Chalfie']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list indexed terms for field \"file_content\"\n",
    "[term for term in myReader.field_terms(\"file_content\")][1000:1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29729\n"
     ]
    }
   ],
   "source": [
    "#how many terms do we have?\n",
    "print(myReader.field_length(\"file_content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs with 'bit' 1\n",
      "# docs with 'are' 11\n",
      "# docs with 'get' 6\n"
     ]
    }
   ],
   "source": [
    "# how many documents have the phares \"bit\", blob\"\n",
    "#   in the field \"file_content\"?\n",
    "print(\"# docs with 'bit'\", myReader.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'Text',\n",
       " 'Analysis',\n",
       " 'with',\n",
       " 'whoosh.analysis']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we start with basic tokenizer\n",
    "tokenizer = RegexTokenizer()\n",
    "[token.text for token in tokenizer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'ar', 'go', 'to', 'do', 'Text', 'Analysi', 'with', 'whoosh.analysi']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we might want use stemming:\n",
    "stmAnalyzer = RegexTokenizer() | StemFilter()\n",
    "[token.text for token in stmAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'ar', 'go', 'to', 'do', 'text', 'analysi', 'with', 'whoosh.analysi']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We probably want to lower-case it\n",
    "# so we add LowercaseFilter\n",
    "stmLwrAnalyzer = RegexTokenizer() | LowercaseFilter() | StemFilter()\n",
    "[token.text for token in stmLwrAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'do', 'text', 'analysi', 'whoosh.analysi']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we probably want to ignore words like \"we\", \"are\", \"with\" when we index files\n",
    "# so we add StopFilter to filter stop words\n",
    "stmLwrStpAnalyzer = RegexTokenizer() | LowercaseFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'do', 'text', 'analysi', 'whoosh', 'analysi']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also probably want to break phrases like \"whoosh.analysis\" into \"whoosh\" and \"analysis\"\n",
    "# so we add IntraWordFilter\n",
    "stmLwrStpIntraAnalyzer = RegexTokenizer() | LowercaseFilter() | IntraWordFilter() | StopFilter() | StemFilter()\n",
    "[token.text for token in stmLwrStpIntraAnalyzer(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the new analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a Schema with the new analyzer\n",
    "mySchema2 = Schema(file_path = ID(stored=True),\n",
    "                   file_content = TEXT(analyzer = stmLwrStpIntraAnalyzer))\n",
    "\n",
    "# create the index based on the new schema\n",
    "myIndex2 = createIndex(mySchema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already indexed: 5\n",
      "already indexed: 10\n",
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "addFilesToIndex(myIndex2, filesToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a query parser for the field \"file_content\" in the index\n",
    "myQueryParser2 = QueryParser(\"file_content\", schema=myIndex2.schema)\n",
    "mySearcher2 = myIndex2.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ret               \t01\t3\n",
      "num_rel               \t01\t3\n",
      "num_rel_ret           \t01\t3\n",
      "map                   \t01\t1.0000\n",
      "Rprec                 \t01\t1.0000\n",
      "bpref                 \t01\t1.0000\n",
      "recip_rank            \t01\t1.0000\n",
      "iprec_at_recall_0.00  \t01\t1.0000\n",
      "iprec_at_recall_0.10  \t01\t1.0000\n",
      "iprec_at_recall_0.20  \t01\t1.0000\n",
      "iprec_at_recall_0.30  \t01\t1.0000\n",
      "iprec_at_recall_0.40  \t01\t1.0000\n",
      "iprec_at_recall_0.50  \t01\t1.0000\n",
      "iprec_at_recall_0.60  \t01\t1.0000\n",
      "iprec_at_recall_0.70  \t01\t1.0000\n",
      "iprec_at_recall_0.80  \t01\t1.0000\n",
      "iprec_at_recall_0.90  \t01\t1.0000\n",
      "iprec_at_recall_1.00  \t01\t1.0000\n",
      "P_5                   \t01\t0.6000\n",
      "P_10                  \t01\t0.3000\n",
      "P_15                  \t01\t0.2000\n",
      "P_20                  \t01\t0.1500\n",
      "P_30                  \t01\t0.1000\n",
      "P_100                 \t01\t0.0300\n",
      "P_200                 \t01\t0.0150\n",
      "P_500                 \t01\t0.0060\n",
      "P_1000                \t01\t0.0030\n",
      "num_ret               \t02\t11\n",
      "num_rel               \t02\t8\n",
      "num_rel_ret           \t02\t8\n",
      "map                   \t02\t0.9207\n",
      "Rprec                 \t02\t0.8750\n",
      "bpref                 \t02\t0.7500\n",
      "recip_rank            \t02\t1.0000\n",
      "iprec_at_recall_0.00  \t02\t1.0000\n",
      "iprec_at_recall_0.10  \t02\t1.0000\n",
      "iprec_at_recall_0.20  \t02\t1.0000\n",
      "iprec_at_recall_0.30  \t02\t1.0000\n",
      "iprec_at_recall_0.40  \t02\t1.0000\n",
      "iprec_at_recall_0.50  \t02\t1.0000\n",
      "iprec_at_recall_0.60  \t02\t0.8750\n",
      "iprec_at_recall_0.70  \t02\t0.8750\n",
      "iprec_at_recall_0.80  \t02\t0.8750\n",
      "iprec_at_recall_0.90  \t02\t0.8000\n",
      "iprec_at_recall_1.00  \t02\t0.8000\n",
      "P_5                   \t02\t0.8000\n",
      "P_10                  \t02\t0.8000\n",
      "P_15                  \t02\t0.5333\n",
      "P_20                  \t02\t0.4000\n",
      "P_30                  \t02\t0.2667\n",
      "P_100                 \t02\t0.0800\n",
      "P_200                 \t02\t0.0400\n",
      "P_500                 \t02\t0.0160\n",
      "P_1000                \t02\t0.0080\n",
      "num_ret               \t03\t2\n",
      "num_rel               \t03\t1\n",
      "num_rel_ret           \t03\t1\n",
      "map                   \t03\t1.0000\n",
      "Rprec                 \t03\t1.0000\n",
      "bpref                 \t03\t1.0000\n",
      "recip_rank            \t03\t1.0000\n",
      "iprec_at_recall_0.00  \t03\t1.0000\n",
      "iprec_at_recall_0.10  \t03\t1.0000\n",
      "iprec_at_recall_0.20  \t03\t1.0000\n",
      "iprec_at_recall_0.30  \t03\t1.0000\n",
      "iprec_at_recall_0.40  \t03\t1.0000\n",
      "iprec_at_recall_0.50  \t03\t1.0000\n",
      "iprec_at_recall_0.60  \t03\t1.0000\n",
      "iprec_at_recall_0.70  \t03\t1.0000\n",
      "iprec_at_recall_0.80  \t03\t1.0000\n",
      "iprec_at_recall_0.90  \t03\t1.0000\n",
      "iprec_at_recall_1.00  \t03\t1.0000\n",
      "P_5                   \t03\t0.2000\n",
      "P_10                  \t03\t0.1000\n",
      "P_15                  \t03\t0.0667\n",
      "P_20                  \t03\t0.0500\n",
      "P_30                  \t03\t0.0333\n",
      "P_100                 \t03\t0.0100\n",
      "P_200                 \t03\t0.0050\n",
      "P_500                 \t03\t0.0020\n",
      "P_1000                \t03\t0.0010\n",
      "num_ret               \t04\t8\n",
      "num_rel               \t04\t5\n",
      "num_rel_ret           \t04\t5\n",
      "map                   \t04\t0.8393\n",
      "Rprec                 \t04\t0.6000\n",
      "bpref                 \t04\t0.8400\n",
      "recip_rank            \t04\t1.0000\n",
      "iprec_at_recall_0.00  \t04\t1.0000\n",
      "iprec_at_recall_0.10  \t04\t1.0000\n",
      "iprec_at_recall_0.20  \t04\t1.0000\n",
      "iprec_at_recall_0.30  \t04\t1.0000\n",
      "iprec_at_recall_0.40  \t04\t1.0000\n",
      "iprec_at_recall_0.50  \t04\t1.0000\n",
      "iprec_at_recall_0.60  \t04\t1.0000\n",
      "iprec_at_recall_0.70  \t04\t0.6250\n",
      "iprec_at_recall_0.80  \t04\t0.6250\n",
      "iprec_at_recall_0.90  \t04\t0.6250\n",
      "iprec_at_recall_1.00  \t04\t0.6250\n",
      "P_5                   \t04\t0.6000\n",
      "P_10                  \t04\t0.5000\n",
      "P_15                  \t04\t0.3333\n",
      "P_20                  \t04\t0.2500\n",
      "P_30                  \t04\t0.1667\n",
      "P_100                 \t04\t0.0500\n",
      "P_200                 \t04\t0.0250\n",
      "P_500                 \t04\t0.0100\n",
      "P_1000                \t04\t0.0050\n",
      "num_ret               \t05\t2\n",
      "num_rel               \t05\t2\n",
      "num_rel_ret           \t05\t0\n",
      "map                   \t05\t0.0000\n",
      "Rprec                 \t05\t0.0000\n",
      "bpref                 \t05\t0.0000\n",
      "recip_rank            \t05\t0.0000\n",
      "iprec_at_recall_0.00  \t05\t0.0000\n",
      "iprec_at_recall_0.10  \t05\t0.0000\n",
      "iprec_at_recall_0.20  \t05\t0.0000\n",
      "iprec_at_recall_0.30  \t05\t0.0000\n",
      "iprec_at_recall_0.40  \t05\t0.0000\n",
      "iprec_at_recall_0.50  \t05\t0.0000\n",
      "iprec_at_recall_0.60  \t05\t0.0000\n",
      "iprec_at_recall_0.70  \t05\t0.0000\n",
      "iprec_at_recall_0.80  \t05\t0.0000\n",
      "iprec_at_recall_0.90  \t05\t0.0000\n",
      "iprec_at_recall_1.00  \t05\t0.0000\n",
      "P_5                   \t05\t0.0000\n",
      "P_10                  \t05\t0.0000\n",
      "P_15                  \t05\t0.0000\n",
      "P_20                  \t05\t0.0000\n",
      "P_30                  \t05\t0.0000\n",
      "P_100                 \t05\t0.0000\n",
      "P_200                 \t05\t0.0000\n",
      "P_500                 \t05\t0.0000\n",
      "P_1000                \t05\t0.0000\n",
      "num_ret               \t06\t3\n",
      "num_rel               \t06\t2\n",
      "num_rel_ret           \t06\t2\n",
      "map                   \t06\t0.8333\n",
      "Rprec                 \t06\t0.5000\n",
      "bpref                 \t06\t1.0000\n",
      "recip_rank            \t06\t1.0000\n",
      "iprec_at_recall_0.00  \t06\t1.0000\n",
      "iprec_at_recall_0.10  \t06\t1.0000\n",
      "iprec_at_recall_0.20  \t06\t1.0000\n",
      "iprec_at_recall_0.30  \t06\t1.0000\n",
      "iprec_at_recall_0.40  \t06\t1.0000\n",
      "iprec_at_recall_0.50  \t06\t1.0000\n",
      "iprec_at_recall_0.60  \t06\t0.6667\n",
      "iprec_at_recall_0.70  \t06\t0.6667\n",
      "iprec_at_recall_0.80  \t06\t0.6667\n",
      "iprec_at_recall_0.90  \t06\t0.6667\n",
      "iprec_at_recall_1.00  \t06\t0.6667\n",
      "P_5                   \t06\t0.4000\n",
      "P_10                  \t06\t0.2000\n",
      "P_15                  \t06\t0.1333\n",
      "P_20                  \t06\t0.1000\n",
      "P_30                  \t06\t0.0667\n",
      "P_100                 \t06\t0.0200\n",
      "P_200                 \t06\t0.0100\n",
      "P_500                 \t06\t0.0040\n",
      "P_1000                \t06\t0.0020\n",
      "runid                 \tall\ttest\n",
      "num_q                 \tall\t6\n",
      "num_ret               \tall\t29\n",
      "num_rel               \tall\t21\n",
      "num_rel_ret           \tall\t19\n",
      "map                   \tall\t0.7656\n",
      "gm_map                \tall\t0.1364\n",
      "Rprec                 \tall\t0.6625\n",
      "bpref                 \tall\t0.7650\n",
      "recip_rank            \tall\t0.8333\n",
      "iprec_at_recall_0.00  \tall\t0.8333\n",
      "iprec_at_recall_0.10  \tall\t0.8333\n",
      "iprec_at_recall_0.20  \tall\t0.8333\n",
      "iprec_at_recall_0.30  \tall\t0.8333\n",
      "iprec_at_recall_0.40  \tall\t0.8333\n",
      "iprec_at_recall_0.50  \tall\t0.8333\n",
      "iprec_at_recall_0.60  \tall\t0.7569\n",
      "iprec_at_recall_0.70  \tall\t0.6944\n",
      "iprec_at_recall_0.80  \tall\t0.6944\n",
      "iprec_at_recall_0.90  \tall\t0.6819\n",
      "iprec_at_recall_1.00  \tall\t0.6819\n",
      "P_5                   \tall\t0.4333\n",
      "P_10                  \tall\t0.3167\n",
      "P_15                  \tall\t0.2111\n",
      "P_20                  \tall\t0.1583\n",
      "P_30                  \tall\t0.1056\n",
      "P_100                 \tall\t0.0317\n",
      "P_200                 \tall\t0.0158\n",
      "P_500                 \tall\t0.0063\n",
      "P_1000                \tall\t0.0032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trecEval(TOPIC_FILE, QRELS_FILE, myQueryParser2, mySearcher2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# docs with 'bit' 11\n",
      "# docs with 'are' 0\n",
      "# docs with 'get' 7\n"
     ]
    }
   ],
   "source": [
    "# let count the same words again\n",
    "myReader2 = myIndex2.reader()\n",
    "print(\"# docs with 'bit'\", myReader2.doc_frequency(\"file_content\", \"bit\"))\n",
    "print(\"# docs with 'are'\", myReader2.doc_frequency(\"file_content\", \"are\"))\n",
    "print(\"# docs with 'get'\", myReader2.doc_frequency(\"file_content\", \"get\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you explain the differences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK's stemmers and lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download required resources\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we'll compare two stemmers and a lemmatizer\n",
    "lrStem = LancasterStemmer()\n",
    "sbStem = SnowballStemmer(\"english\")\n",
    "wnLemm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a list of words to compare the stemmers on\n",
    "listWords = [\"going\", \"saying\", \"minimize\", \"maximum\", \n",
    "             \"meeting\", \"files\", \"tries\", \"is\", \"are\", \"beautiful\",\n",
    "             \"summarize\", \"better\", \"dogs\", \"phenomena\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          going              go           going              go\n",
      "            say             say          saying             say\n",
      "          minim           minim        minimize        minimize\n",
      "          maxim         maximum         maximum         maximum\n",
      "           meet            meet         meeting            meet\n",
      "            fil            file            file            file\n",
      "            tri             tri             try             try\n",
      "             is              is              is              be\n",
      "             ar             are             are              be\n",
      "         beauty          beauti       beautiful       beautiful\n",
      "           summ          summar       summarize       summarize\n",
      "            bet          better          better          better\n",
      "            dog             dog             dog             dog\n",
      "       phenomen       phenomena      phenomenon       phenomena\n"
     ]
    }
   ],
   "source": [
    "for word in listWords:\n",
    "    print(\"%15s %15s %15s %15s\" % (lrStem.stem(word),\n",
    "                                   sbStem.stem(word),\n",
    "                                   wnLemm.lemmatize(word),\n",
    "                                   wnLemm.lemmatize(word, 'v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use NLTK stemmers / lemmatizers in Whoosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dont change this! Use it as-is in your code\n",
    "# This filter will run for both the index and the query\n",
    "from whoosh.analysis import Filter\n",
    "class CustomFilter(Filter):\n",
    "    is_morph = True\n",
    "    def __init__(self, filterFunc, *args, **kwargs):\n",
    "        self.customFilter = filterFunc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    def __eq__(self):\n",
    "        return (other\n",
    "                and self.__class__ is other.__class__)\n",
    "    def __call__(self, tokens):\n",
    "        for t in tokens:\n",
    "            if t.mode == 'query': # if called by query parser\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t\n",
    "            else: # == 'index' if called by indexer\n",
    "                t.text = self.customFilter(t.text, *self.args, **self.kwargs)\n",
    "                yield t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'ar', 'going', 'to', 'do', 'text', 'analys', 'with', 'whoosh.analysis']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example1: Whoosh filter for NLTK's LancasterStemmer\n",
    "myFilter1 = RegexTokenizer() | CustomFilter(LancasterStemmer().stem)\n",
    "[token.text for token in myFilter1(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'do',\n",
       " 'Text',\n",
       " 'Analysis',\n",
       " 'with',\n",
       " 'whoosh.analysis']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example2: Whoosh filter for NLTK's WordNetLemmatizer\n",
    "myFilter2 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize)\n",
    "[token.text for token in myFilter2(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'be', 'go', 'to', 'do', 'Text', 'Analysis', 'with', 'whoosh.analysis']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example3: Whoosh filter for NLTK's WordNetLemmatizer for verbs\n",
    "myFilter3 = RegexTokenizer() | CustomFilter(WordNetLemmatizer().lemmatize, 'v')\n",
    "[token.text for token in myFilter3(\"We are going to do Text Analysis with whoosh.analysis\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use myFilter1/2/3 as part of your Schema\n",
    "\n",
    "------------\n",
    "You can find details of other NLTK Stemmers and Lemmatizers here:\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Additional Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "And([Term('file_content', 'University'), Term('file_content', 'of'), Term('file_content', 'Toronto')])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myQueryParser.parse(\"University of Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Or([Term('file_content', 'University'), Term('file_content', 'of'), Term('file_content', 'Toronto')])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myQueryParser.parse(\"University OR of OR Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Or([Term('file_content', 'univers'), Term('file_content', 'toronto')])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myQueryParser2.parse(\"University OR of OR Toronto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Or([And([Term('file_content', 'University'), Term('file_content', 'Toronto')]), And([Term('file_content', 'University'), Term('file_content', 'British'), Term('file_content', 'Columbia')])])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myQueryParser.parse(\"(University Toronto) OR (University British Columbia)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleQueryGo = myQueryParser2.parse(\"go\")\n",
    "sampleQueryResultsGo = mySearcher2.search(sampleQueryGo, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email04 0 1.532818707775775\n",
      "email08 1 1.4606112073704993\n",
      "email09 2 1.4308213268867864\n",
      "email05 3 1.307224528544008\n",
      "email03 4 1.2392797101539106\n",
      "email07 5 1.2392797101539106\n",
      "email06 6 1.227396210411099\n",
      "email14 7 0.9206709471157581\n",
      "email01 8 0.8960772103589507\n",
      "email02 9 0.8960772103589507\n",
      "email10 10 0.8712000019125419\n"
     ]
    }
   ],
   "source": [
    "# inspect the result:\n",
    "# for each document print the rank and the score\n",
    "for (docnum, result) in enumerate(sampleQueryResultsGo):\n",
    "    score = sampleQueryResultsGo.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh import scoring\n",
    "mySearcher2TF = myIndex2.searcher(weighting=scoring.TF_IDF())\n",
    "sampleQueryResultsGoTF = mySearcher2TF.search(sampleQueryGo, limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email04 0 3.651954492041481\n",
      "email08 1 2.7389658690311105\n",
      "email09 2 2.7389658690311105\n",
      "email03 3 1.8259772460207404\n",
      "email05 4 1.8259772460207404\n",
      "email06 5 1.8259772460207404\n",
      "email07 6 1.8259772460207404\n",
      "email01 7 0.9129886230103702\n",
      "email02 8 0.9129886230103702\n",
      "email10 9 0.9129886230103702\n",
      "email14 10 0.9129886230103702\n"
     ]
    }
   ],
   "source": [
    "for (docnum, result) in enumerate(sampleQueryResultsGoTF):\n",
    "    score = sampleQueryResultsGoTF.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change BM25F parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "email04 0 1.532818707775775\n",
      "email08 1 1.4606112073704993\n",
      "email09 2 1.4308213268867864\n",
      "email05 3 1.307224528544008\n",
      "email03 4 1.2392797101539106\n",
      "email07 5 1.2392797101539106\n",
      "email06 6 1.227396210411099\n",
      "email14 7 0.9206709471157581\n",
      "email01 8 0.8960772103589507\n",
      "email02 9 0.8960772103589507\n",
      "email10 10 0.8712000019125419\n"
     ]
    }
   ],
   "source": [
    "mySearcher2BM = myIndex2.searcher(weighting=scoring.BM25F(B=0.75, K1=1.2))\n",
    "sampleQueryResultsGoBM = mySearcher2BM.search(sampleQueryGo, limit=None)\n",
    "\n",
    "for (docnum, result) in enumerate(sampleQueryResultsGoBM):\n",
    "    score = sampleQueryResultsGoBM.score(docnum)\n",
    "    fileName = os.path.basename(result[\"file_path\"])\n",
    "    print(fileName, docnum, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store additional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: from exgw1-cbr.nexus.csiro.au ([152.83.3.66]) by EXACTN1-CBR.nexus.csiro.au with Microsoft SMTPSVC(5.0.2195.6713);\n",
      "\t Fri, 18 Apr 2008 23:24:14 +1000\n",
      "Received: from act-MTAout6.csiro.au ([150.229.7.43]) by exgw1-cbr.nexus.csiro.au with Microsoft SMTPSVC(5.0.2195.6713);\n",
      "\t Fri, 18 Apr 2008 23:24:14 +1000\n",
      "X-SBRS: 6.3\n",
      "X-IronPort-Anti-Spam-Filtered: true\n",
      "X-IronPort-Anti-Spam-Result: AtMDABo9CEiAZ2D+bmdsb2JhbACRY5oWPw\n",
      "X-IronPort-AV: E=Sophos;i=\"4.25,677,1199624400\"; \n",
      "   d=\"scan'208\";a=\"191215222\"\n",
      "Received: from anumail6.anu.edu.au ([130.56.64.140])\n",
      "  by act-ironport-ldap.csiro.au with ESMTP/TLS/DHE-RSA-AES256-SHA; 18 Apr 2008 23:24:14 +1000\n",
      "Received: from chem.harvard.edu (chem.harvard.edu [128.103.96.254])\n",
      "\tby anumail6.anu.edu.au (8.13.8/8.13.8) with ESMTP id m3IDO6B8020975\n",
      "\tfor <paul.thomas@anu.edu.au>; Fri, 18 Apr 2008 23:24:11 +1000 (EST)\n",
      "\t(envelope-from mini-air-bounces@chem.harvard.edu)\n",
      "Received: from chem.harvard.edu (chem.harvard.edu [127.0.0.1])\n",
      "\tby chem.harvard.edu (Postfix) with ESMTP id 6F78E113F97;\n",
      "\tFri, 18 Apr 2008 09:07:16 -0400 (EDT)\n",
      "X-Original-To: mini-air@air.harvard.edu\n",
      "Delivered-To: mini-air@air.harvard.edu\n",
      "Received: from QMTA02.emeryville.ca.mail.comcast.net\n",
      "\t(qmta02.emeryville.ca.mail.comcast.net [76.96.30.24])\n",
      "\tby chem.harvard.edu (Postfix) with ESMTP id 9B0D1113D2D\n",
      "\tfor <mini-air@air.harvard.edu>; Fri, 18 Apr 2008 09:06:19 -0400 (EDT)\n",
      "Received: from OMTA07.emeryville.ca.mail.comcast.net ([76.96.30.59])\n",
      "\tby QMTA02.emeryville.ca.mail.comcast.net with comcast\n",
      "\tid F0LA1Z0071GXsucA209M00; Fri, 18 Apr 2008 13:04:43 +0000\n",
      "Received: from [192.168.1.100] ([76.19.64.99])\n",
      "\tby OMTA07.emeryville.ca.mail.comcast.net with comcast\n",
      "\tid F16B1Z00B28Uzto8T00000; Fri, 18 Apr 2008 13:06:13 +0000\n",
      "X-Authority-Analysis: v=1.0 c=1 a=0dSSCedAPYgA:10 a=P5jqPY5q80UA:10\n",
      "\ta=jIfanU7UAAAA:8 a=CjxXgO3LAAAA:8 a=6D-Hy7tQAAAA:8 a=OPzIfofIAAAA:8\n",
      "\ta=UVq9kkz1AAAA:8 a=vyo-Ye8WAAAA:8 a=uWpNjx2UBVi2Eln7TlgA:9\n",
      "\ta=Jre56OS1aD8LbeU8zggA:7 a=wBhMT9-iv29N96ea_5Q-fcWN69EA:4\n",
      "\ta=QhG1sEnpFG8A:10\n",
      "\ta=viksUPKxHHcA:10 a=_q8Q0QWmO9cA:10 a=ptXlZgw1ZlIA:10 a=GPg30UNevoAA:10\n",
      "\ta=Le0zlXOLzSIA:10 a=x5XM52EWLrgA:10 a=0hygJ-gJBKgA:10 a=mbQ1mHQZ8V0A:10\n",
      "\ta=dclxZDl7ickA:10 a=FWdE53RHWygA:10\n",
      "Message-Id: <5956A908-FFD2-4FC1-BF6F-28EBA2F0AC9E@chem2.harvard.edu>\n",
      "From: Marc Abrahams <marca@chem2.harvard.edu>\n",
      "To: mini-air@chem.harvard.edu\n",
      "Mime-Version: 1.0 (Apple Message framework v919.2)\n",
      "Date: Fri, 18 Apr 2008 09:06:11 -0400\n",
      "X-Mailer: Apple Mail (2.919.2)\n",
      "Subject: [Mini-air] April mini-AIR - Kluges, the life cycle,\n",
      "\tmusic response in mental\n",
      "X-BeenThere: mini-air@air.harvard.edu\n",
      "X-Mailman-Version: 2.1.8\n",
      "Precedence: list\n",
      "List-Id: The mini-Annals of Improbable Research <mini-air.air.harvard.edu>\n",
      "List-Unsubscribe: <http://chem.harvard.edu/mailman/listinfo/mini-air>,\n",
      "\t<mailto:mini-air-request@air.harvard.edu?subject=unsubscribe>\n",
      "List-Archive: <http://chem.harvard.edu/pipermail/mini-air>\n",
      "List-Post: <mailto:mini-air@air.harvard.edu>\n",
      "List-Help: <mailto:mini-air-request@air.harvard.edu?subject=help>\n",
      "List-Subscribe: <http://chem.harvard.edu/mailman/listinfo/mini-air>,\n",
      "\t<mailto:mini-air-request@air.harvard.edu?subject=subscribe>\n",
      "Content-Type: text/plain;\n",
      "  charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding: quoted-printable\n",
      "Sender: mini-air-bounces@chem.harvard.edu\n",
      "Errors-To: mini-air-bounces@chem.harvard.edu\n",
      "X-PMX-Version: 5.4.1.325704, Antispam-Engine: 2.6.0.325393, Antispam-Data: 2008.4.18.60358 external\n",
      "X-PerlMx-Spam: Gauge=X, Probability=10%, Report='LINES_OF_YELLING_3 0.671, BODY_SIZE_10000_PLUS 0, ECARD_KNOWN_DOMAINS 0, __CP_NAME_BODY 0, __CP_NOT_1 0, __CP_URI_IN_BODY 0, __CT 0, __CTE 0, __CT_TEXT_PLAIN 0, __FRAUD_419_BADTHINGS 0, __HAS_LIST_HEADER 0, __HAS_LIST_HELP 0, __HAS_LIST_SUBSCRIBE 0, __HAS_LIST_UNSUBSCRIBE 0, __HAS_MSGID 0, __HAS_X_MAILER 0, __LINES_OF_YELLING 0, __MIME_TEXT_ONLY 0, __MIME_VERSION 0, __SANE_MSGID 0'\n",
      "Return-Path: mini-air-bounces@chem.harvard.edu\n",
      "X-OriginalArrivalTime: 18 Apr 2008 13:24:14.0673 (UTC) FILETIME=[7F1A8410:01C8A157]\n",
      "X-Length: 20224\n",
      "X-UID: 6709\n",
      "Status: R\n",
      "X-Status: N\n",
      "X-KMail-EncryptionState:  \n",
      "X-KMail-SignatureState:  \n",
      "X-KMail-MDN-Sent:  \n",
      "\n",
      "PLEASE FORWARD/POST AS APPROPRIATE\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D=3D=3D=3D\n",
      "The mini-Annals of Improbable Research (\"mini-AIR\")\n",
      "April 2008, Issue number 2008-04. ISSN 1076-500X.\n",
      "----------------------------------------------------------\n",
      "A free newsletter of tidbits too tiny to fit in\n",
      "Annals of Improbable Research (AIR)\n",
      "      This issue at\n",
      "      <http://www.improbable.com/airchives/miniair/2008/mini2008-04.htm>\n",
      "      Archive at <http://www.improbable.com/magazine/>\n",
      "Key words: improbable research, science humor, Ig Nobel, AIR, the\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D=3D=3D=3D\n",
      "\n",
      "-----------------------------\n",
      "2008-04-01 TABLE OF CONTENTS\n",
      "\n",
      "2008-04-02 Imminent Events\n",
      "2008-04-03 What's New in the Magazine\n",
      "2008-04-04 Sentence-of-Death: Kluge's Masterpiece\n",
      "2008-04-05 Great Research Kluges\n",
      "2008-04-06 Journals for Your Life Cycle\n",
      "2008-04-07 Plea for Incompetence\n",
      "2008-04-08 Pithy Thought of the Month\n",
      "2008-04-09 Why Bedouin Robes Are Not Gray, Perhaps\n",
      "2008-04-10 Shuffling/Hitching/Scooting/Sliding Poet\n",
      "2008-04-11 Peat-Bog Man's Intestines Competition\n",
      "2008-04-12 RESEARCH SPOTLIGHT: Music Response in a Mental Asylum\n",
      "2008-04-13 BLOGLIGHTS: Promiscuous Streets, Clowns, Colors\n",
      "2008-04-14 MAY WE RECOMMEND: Sexual Addiction and Overdenture\n",
      "2008-04-15 Improbable Research Events\n",
      "2008-04-16 -- How to Subscribe to AIR (*)\n",
      "2008-04-17 -- Our Address (*)\n",
      "2008-04-18 -- Please Forward/Post This Issue! (*)\n",
      "2008-04-19 -- How to Receive mini-AIR, etc. (*)\n",
      "\n",
      "      Items marked (*) are reprinted in every issue.\n",
      "\n",
      "      mini-AIR is\n",
      "      a free monthly *e-supplement* to the print magazine\n",
      "      Annals of Improbable Research\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-02 Imminent Events\n",
      "\n",
      "      CAMBRIDGE, MA  Mon, Apr 21, 7 pm. Free.\n",
      "      Skeptics in the Pub, at Asgard, 350 Mass Ave.\n",
      "      http://upcoming.yahoo.com/event/470373\n",
      "\n",
      "      PHILADELPHIA   Wed, Apr 23, noon. Free.\n",
      "      Chemical Heritage Foundation - Brown Bag Lecture\n",
      "      315 Chestnut Street, 6th Floor Conference Room\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-03 What's New in the Magazine\n",
      "\n",
      "The entire magazine -- the Annals of Improbable Research -- is\n",
      "now online for free for all. Open access, open access, we repeat.\n",
      "\n",
      "The March/April 2008 issue (vol. 14, no. 2) is a special Writing\n",
      "Research issue. Highlights include:\n",
      "\n",
      "<> \"How to Write 85,000 (or maybe 200,000) Books\"\n",
      "<> \"How to Write an Interdisciplinary Research Paper: Planning\n",
      "for Retirement by Solving Time Travel Paradoxes Using Open Book\n",
      "Management in Nearby Disk Galaxies,\" Eric Schulman, Eric\n",
      "Schulman, Eric Schulman, and Eric Schulman\n",
      "<> \"Hard Looks at Doctors' Handwriting\"\n",
      "\n",
      "It and many back issues are online at\n",
      "<http://www.improbable.com/magazine/>\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-04 Sentence-of-Death: Kluge's Masterpiece\n",
      "\n",
      "The Sentence-of-Death project grinds on. Investigator Blanton\n",
      "Daves writes to remind us that it has been ten years since we\n",
      "presented E. Kluge's classic sentence-of-death.\n",
      "\n",
      "In tribute, we present it again. Investigator David Rind alerted\n",
      "us to the study \"Health Information, Privacy, Confidentiality and\n",
      "Ethics,\" E.H. Kluge, International Journal of Bio-Medical\n",
      "Computing, vol. 35, supplement 1, 1994, pp. 23-7. The study's\n",
      "author, Eike-Henner Kluge, at the University of Victoria, Canada,\n",
      "begins his abstract with this sentence-of-death:\n",
      "\n",
      "\"Electronic patient records are becoming tologically reified\n",
      "entities that play the role of epistemic patient analogues in\n",
      "information space.\"\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-05 Great Research Kluges\n",
      "\n",
      "Here are pointers to some exploits by other great Kluges of the first\n",
      "type:\n",
      "\n",
      "\"The mere fact that so distinguished a scholar as Professor Kluge could\n",
      "edit a practically unaltered version of a Chaucer Grammar and\n",
      "Prosody...\" <http://tinyurl.com/3nq6kz>\n",
      "\n",
      "\"Professor Kluge has confidence in cladograms...\"\n",
      "<http://tinyurl.com/45wn3t>\n",
      "\n",
      "<Professor Kluge has served as editor of the German literature journal\n",
      "Monatshefte...\" <http://tinyurl.com/45z3mc>\n",
      "\n",
      "<Professor Kluge has written a beautifully written and insightful look\n",
      "at the educational process at a small liberal arts college...>\n",
      "<http://tinyurl.com/3v8cuc>\n",
      "\n",
      "\"Professor Kluge will visit MSL on Tuesday, 2 September, to discuss a\n",
      "possible collaboration in the future accelerator project...\"\n",
      "<http://tinyurl.com/4om2vd>\n",
      "\n",
      "\"Professor Kluge prescribes a quantity of Epsom salts...\"\n",
      "<http://tinyurl.com/4bsx6f>\n",
      "\n",
      "Kluge is of course not only a noble name, but also a noble, and perhaps\n",
      "unrelated, word. One of the word's definitions, put (or perhaps\n",
      "repeated?) into words by celebrity hacker Eric Raymond\n",
      "<http://tinyurl.com/6gqclf>, is:\n",
      "\n",
      "     \"A clever programming trick\n",
      "     intended to solve a particular nasty case\n",
      "     in an expedient, if not clear, manner.\n",
      "     Often used to repair bugs.\"\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-06 Journals for Your Life Cycle\n",
      "\n",
      "Reto Schneider, author of the book Das Buch der Verr=FCckten\n",
      "Experimente, compiled a list of journals that follow you through\n",
      "life:\n",
      "\n",
      "=ABThe Birth Gazette=BB\n",
      "=ABGifted Child Quarterly=BB\n",
      "=ABJournal of Child Psychotherapy=BB\n",
      "=ABChild and Adolescent Psychopharmacology News=BB\n",
      "=ABJournal of Adolescence=BB\n",
      "=ABJournal of Drug Education=BB\n",
      "=ABJournal of Marriage and Family=BB\n",
      "=ABJournal of Happiness Studies=BB\n",
      "=ABJournal of Marital and Family Therapy=BB\n",
      "=ABHeadache Quarterly=BB\n",
      "=ABJournal of Worry Research=BB\n",
      "=ABJournal of Divorce & Remarriage=BB\n",
      "=ABHealthy Weight Journal=BB\n",
      "=ABAgeing International=BB\n",
      "=ABContemporary Gerontology=BB\n",
      "=ABJournal of Alzheimer's Disease=BB\n",
      "=ABArchives of Suicide Research=BB\n",
      "=ABJournal of Near-Death Studies=BB\n",
      "=ABDeath Studies=BB\n",
      "\n",
      "More of Schneider's work is at\n",
      "<http://www.verrueckte-experimente.de/>\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-07 Plea for Incompetence\n",
      "\n",
      "You may have missed our plea for incompetence - specifically for\n",
      "social scientists to reap and study the rare bounty that's\n",
      "available right now. The entire tract is at\n",
      "<http://improbable.com/2008/03/23/social-scientists-alert-gather-that-data/ =\n",
      "\n",
      " >.\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-08 Pithy Thought of the Month\n",
      "\n",
      "\"That we are smarter than algae is a given. Whether we are wiser\n",
      "remains an open question.\"\n",
      "\n",
      "-Robert Frenay, in the book Pulse, page 440.\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-09 Why Bedouin Robes Are Not Gray, Perhaps\n",
      "\n",
      "Investigator Mike Adams writes, about the report \"Why Do Bedouins\n",
      "Wear Black Robes\":\n",
      "\n",
      "      ***\n",
      "Many years ago I heard Knut Schmidt-Nielsen, famous for his work\n",
      "on adaptations to desert conditions, talk about this. He said\n",
      "that he finally asked the Bedouin why they used black wool. Their\n",
      "answer was 'all of our goats are black'. So, if the story is\n",
      "true, they didn't see the decision as one of choice.\n",
      "      ***\n",
      "\n",
      "Note: He was referring to:\n",
      "<http://improbable.com/2006/08/17/guardian-column-27/>\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-10 Shuffling/Hitching/Scooting/Sliding Poet\n",
      "\n",
      "The judges have chosen a winner for last month's\n",
      "Shuffling/Hitching/Scooting/Sliding Limerick Competition, which\n",
      "asked for a limerick to honor the study \"Shuffling, Hitching,\n",
      "Scooting or Sliding: Some Observations in 30 Otherwise Normal\n",
      "Children,\" R. Robson, Developmental Medicine and Child Neurology,\n",
      "vol. 12, 1970, pp. 608-17.\n",
      "\n",
      "The winner and her limerick are:\n",
      "\n",
      "INVESTIGATOR ALANNA COCTOKIN:\n",
      "Their normal behavior aside,\n",
      "They scoot and they shuffle or slide.\n",
      "    Whenever they swerved,\n",
      "    Doc Robson observed\n",
      "Who moved, and with what kind of stride.\n",
      "\n",
      "AND HERE IS THE ASSESSMENT FROM LIMERICK LAUREATE MARTIN EIGER:\n",
      "The kids in this short publication\n",
      "Are normal, save one aberration.\n",
      "    They shuffle and hitch,\n",
      "    Scoot and slide, all of which\n",
      "Subjects them to some observation.\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-11 Peat-Bog Man's Intestines Competition\n",
      "\n",
      "Peat-Bog Man's intestines are the subject of this month's\n",
      "limerick competition. To enter, compose an original limerick that\n",
      "illuminates the nature of this report:\n",
      "\n",
      "                * * *\n",
      "\n",
      "\"The Intestines of a More Than 2000 Years Old Peat-Bog Man:\n",
      "Microscopy, Magnetic Resonance Imaging and 14C-dating,\" Hans\n",
      "St=F8dkilde-J=F8rgensena, Niels Otto Jacobsenb, Esbern Warnckec and\n",
      "Jan Heinemeierd, Journal of Archaeological Science, vol. 35, no.\n",
      "3, March 2008, pp. 530-4.\n",
      "<http://dx.doi.org/10.1016/j.jas.2007.05.010>\n",
      "(Thanks to Tom Gill for bringing this to our attention.) The\n",
      "authors, at Aarhus University, report:\n",
      "\n",
      "\"The intestines of Grauballe Man, who is a 2400-old late Iron Age\n",
      "body found in a raised bog in the central part of Jutland,\n",
      "Denmark, was examined by microscopy and magnetic resonance\n",
      "imaging. Plant roots found in the tissue were radiocarbon dated\n",
      "by AMS.... The low signal spots observed by MRI turned out to be\n",
      "formed by plant roots penetrating the tissue. AMS 14C-dating of\n",
      "these indicated that penetration appeared not more than 200 years\n",
      "after the body was laid in the bog.\"\n",
      "\n",
      "                * * *\n",
      "\n",
      "RULES: Please make sure your rhymes actually do, and that your\n",
      "poem is in classic, trips-off-the-tongue limerick form.\n",
      "\n",
      "PRIZE: The winning poet will receive (if we manage to send it to\n",
      "the correct address) a free, possibly peaty issue of the Annals\n",
      "of Improbable Research. Send entries (one entry per entrant) to:\n",
      "\n",
      "      PEAT-BOG MAN'S INTESTINE LIMERICK COMPETITION\n",
      "      c/o <marca AT chem2.harvard.edu>\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-12 RESEARCH SPOTLIGHT: Music Response in a Mental Asylum\n",
      "\n",
      "This month's specially selected study is:\n",
      "\n",
      "\"A Comparison of the Effects of Hard Rock and Easy Listening on\n",
      "the Frequency of Observed Inappropriate Behaviors: Control of\n",
      "Environmental Antecedents in a Large Public Area,\" Clarke S.\n",
      "Harris, Richard J. Bradley and Sharon K. Titus, Journal of Music\n",
      "Therapy, vol. 29,  no. 1, Spring 1992, pp. 6-17. (Thanks to Jim\n",
      "Cowdery for bringing this to our attention.) The authors explain\n",
      "that their report:\n",
      "\n",
      "\"Compares the inappropriate behavior of clients in an open\n",
      "courtyard at a state mental hospital when hard rock and rap music\n",
      "were played (21 days), followed by easy listening and country\n",
      "music (21 days).... More inappropriate behavior was observed when\n",
      "hard rock and rap music were played than when easy listening and\n",
      "country western music were played.\"\n",
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "2008-04-13 BLOGLIGHTS: Promiscuous Streets, Clowns, Colors\n",
      "\n",
      "Here are some recent topics in our blog:\n",
      "\n",
      "<> The promiscuous streets of Edinburgh\n",
      "<> Discarded underwear + paper production =3D literacy\n",
      "<> Judgment on clowns\n",
      "<> The truth behind the Bozo Van\n",
      "<> Calculator for the incalculatable\n",
      "\n",
      "and some from the newspaper column in The Guardian:\n",
      "\n",
      "<> The Colours of Indian Bureaucracy\n",
      "<> The promiscuous streets of Edinburgh\n",
      "<> Wassersuggiana (parts 1 and 2)\n",
      "\n",
      "      ... and others\n",
      "\n",
      "      Read the blog\n",
      "      every day at <http://www.improbable.com>\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "2008-04-14 MAY WE RECOMMEND: Sexual Addiction and Overdenture\n",
      "\n",
      "LOVE YOUR WORK\n",
      "\"It's a Wonderful Profession: What the Journal Contributes,\"\n",
      "David L. Delmonico, Sexual Addiction and Compulsivity, vol. 10,\n",
      "no. 4, October-December 2003, pp. 223-4.\n",
      "\n",
      "HUM ON SPARK-EROSION OVERDENTURE\n",
      "\"Implants and the Maxillary Spark-Erosion Overdenture,\" S.K. Hum,\n",
      "Oral Health, vol. 84, no. 11, November 1994, pp. 9-12.\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "2008-04-15 Improbable Research Events\n",
      "\n",
      "For details and additional events, see\n",
      "<http://improbable.com/improbable-research-shows/complete-schedule>\n",
      "\n",
      "SKEPTICS IN THE PUB, CAMBRIDGE, MA  -- APR 21, 2008\n",
      "\n",
      "CHEMICAL HERITAGE FOUNDATION, PHILADELPHIA    -APR 23, 2008\n",
      "\n",
      "AMERICAN SOCIETY FOR MASS SPECTROMETRY (ASMS) ANNUAL CONFERENCE,\n",
      "DENVER                         -- JUN 1, 2008\n",
      "\n",
      "CHELTENHAM SCIENCE FESTIVAL, UK     -- JUN 7, 2008\n",
      "\n",
      "ALPBACH TECHNOLOGY FORUM, AUSTRIA   -- AUG 21-23, 2008\n",
      "\n",
      "IG NOBEL PRIZE CEREMONY             -- OCT 2, 2008\n",
      "\n",
      "IG INFORMAL LECTURES           -- OCT 4, 2008\n",
      "\n",
      "AMERICAN PHYSICAL SOCIETY, DAYTON, OHIO  OCT 10, 2008\n",
      "\n",
      "\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "--------------------------------------------------------------\n",
      "2008-04-16 -- How to Subscribe to AIR (*)\n",
      "\n",
      "The Annals of Improbable Research is a 6-issues-per-year\n",
      "magazine. (It's bigger and better than the little bits of\n",
      "overflow material you've been reading in this newsletter). The\n",
      "online version is at <http://www.improbable.com/magazine/>.\n",
      "\n",
      "To subscribe to the paper-and-ink version, go to\n",
      "<http://improbable.com/subscribe/> or send in this form:\n",
      "................................................................\n",
      "Name:\n",
      "Address:\n",
      "Address:\n",
      "City and State:\n",
      "Zip or postal code:\n",
      "Country\n",
      "Phone:         FAX:           E-mail:\n",
      ".........................................................\n",
      "SUBSCRIPTIONS (6 issues per year):\n",
      "USA            1 yr/$35       2 yrs/$63\n",
      "Canada/Mexico  1 yr/$42 US    2 yrs/$72 US\n",
      "Overseas       1 yr/$53 US    2 yrs/$97 US\n",
      ".........................................................\n",
      "BACK ISSUES are available, too:\n",
      "<http://www.improbable.com/airchives/paperair/stale.htm>\n",
      ".........................................................\n",
      "Send payment (US bank check, or international money order, or\n",
      "Visa, Mastercard or Discover info) to:\n",
      "      Annals of Improbable Research (AIR)\n",
      "      PO Box 380853, Cambridge, MA 02238 USA\n",
      "      617-491-4437 FAX:617-661-0927 <air AT improbable.com>\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "2008-04-17 -- Our Address (*)\n",
      "\n",
      "Annals of Improbable Research (AIR)\n",
      "PO Box 380853, Cambridge, MA 02238 USA\n",
      "617-491-4437 FAX:617-661-0927\n",
      "\n",
      "EDITORIAL: marca AT chem2.harvard.edu\n",
      "SUBSCRIPTIONS: air AT improbable.com\n",
      "WEB SITE: <http://www.improbable.com>\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "2008-04-18 -- Please Forward/Post This Issue! (*)\n",
      "\n",
      "Please distribute copies of mini-AIR (or excerpts!) wherever\n",
      "appropriate. The only limitations are: A) Please indicate that\n",
      "the material comes from mini-AIR. B) You may NOT distribute mini-\n",
      "AIR for commercial purposes.\n",
      "\n",
      "      ------------- mini-AIRheads -------------\n",
      "EDITOR: Marc Abrahams\n",
      "MINI-PROOFREADER AND PICKER OF NITS (before we introduce the last\n",
      "few at the last moment): Wendy Mattson\n",
      "COMMUTATIVE EDITOR: Stanley Eigen\n",
      "ASSOCIATIVE EDITOR: Mark Dionne\n",
      "PSYCHOLOGY EDITOR: Robin Abrahams\n",
      "CO-CONSPIRATORS: Alice Shirrell Kaswell, Gary Dryfoos, Ernest\n",
      "Ersatz, S. Drew\n",
      "MAITRE DE COMPUTATION: Jerry Lotto\n",
      "AUTHORITY FIGURES: Nobel Laureates Dudley Herschbach, Sheldon\n",
      "Glashow, William Lipscomb, Richard Roberts\n",
      "\n",
      "(c) copyright 2008, Annals of Improbable Research\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "2008-04-19 -- How to Receive mini-AIR, etc. (*)\n",
      "\n",
      "What you are reading right now is mini-AIR. Mini-AIR is a (free!)\n",
      "tiny monthly *supplement* to the bi-monthly print magazine.\n",
      "           ----------------------------\n",
      "To subscribe or unsubscribe, please visit\n",
      "<http://chem.harvard.edu/mailman/listinfo/mini-air>\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=\n",
      "=3D=3D=3D=3D\n",
      "\n",
      "\n",
      "_______________________________________________\n",
      "Mini-air mailing list\n",
      "Mini-air@air.harvard.edu\n",
      "http://chem.harvard.edu/mailman/listinfo/mini-air\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open(\"lab-data/documents/email02\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri, 18 Apr 2008 09:06:11 -0400\n"
     ]
    }
   ],
   "source": [
    "extractedDate = re.search(\"Date: (.*)\\n\", open(\"lab-data/documents/email02\").read()).groups()[0]\n",
    "print(extractedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2008, 4, 18, 9, 6, 11, 0, 1, -1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import email\n",
    "import time\n",
    "email.utils.parsedate(extractedDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import NUMERIC\n",
    "\n",
    "# first, define a Schema for the index\n",
    "mySchemaDate = Schema(file_path = ID(stored=True),\n",
    "                      file_year = NUMERIC(stored=True),\n",
    "                      file_content = TEXT(analyzer = RegexTokenizer()))\n",
    "\n",
    "# now, create the index at the path INDEX_DIR based on the new schema\n",
    "myIndexDate = createIndex(mySchemaDate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addFilesToIndexDate(indexObj, fileList):\n",
    "    # open writer\n",
    "    writer = writing.BufferedWriter(indexObj, period=None, limit=1000)\n",
    "\n",
    "    try:\n",
    "        # write each file to index\n",
    "        for docNum, filePath in enumerate(fileList):\n",
    "            with open(filePath, \"r\", encoding=\"utf-8\") as f:\n",
    "                fileContent = f.read()\n",
    "                parsedYear = email.utils.parsedate(re.search(\"Date: (.*)\\n\", fileContent).groups()[0])[0]\n",
    "                writer.add_document(file_path = filePath,\n",
    "                                    file_year = int(parsedYear),\n",
    "                                    file_content = fileContent)\n",
    "\n",
    "                # print status every 1000 documents\n",
    "                if (docNum+1 % 1000 == 0):\n",
    "                    print(\"already indexed:\", docNum+1)\n",
    "        print(\"done indexing.\")\n",
    "\n",
    "    finally:\n",
    "        # close the index\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done indexing.\n"
     ]
    }
   ],
   "source": [
    "addFilesToIndexDate(myIndexDate, filesToIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myQueryParserDate = QueryParser(\"file_content\", schema=myIndexDate.schema)\n",
    "mySearcherDate = myIndexDate.searcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleQueryDate = myQueryParserDate.parse(\"go\")\n",
    "sampleQueryResultsDate = mySearcherDate.search(sampleQueryDate, limit=None, groupedby=\"file_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'lab-data\\\\documents\\\\email09', 'file_year': 2008}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySearcherDate.stored_fields(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2009: [5, 0], 2008: [8, 10, 9, 1], 2010: [2, 3], 2007: [6, 4, 7]}\n"
     ]
    }
   ],
   "source": [
    "res = sampleQueryResultsDate.groups()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 2009\n",
      "\t5\tlab-data\\documents\\email06\t0.9515367147003012\n",
      "\t0\tlab-data\\documents\\email01\t1.268912431091308\n",
      "Group 2008\n",
      "\t8\tlab-data\\documents\\email09\t0.902826206556481\n",
      "\t10\tlab-data\\documents\\email14\t0.8905624586380126\n",
      "\t9\tlab-data\\documents\\email10\t0.902826206556481\n",
      "\t1\tlab-data\\documents\\email02\t1.257405405496971\n",
      "Group 2010\n",
      "\t2\tlab-data\\documents\\email03\t1.2457191647278192\n",
      "\t3\tlab-data\\documents\\email04\t1.2219953566378796\n",
      "Group 2007\n",
      "\t6\tlab-data\\documents\\email07\t0.9273964639464627\n",
      "\t4\tlab-data\\documents\\email05\t1.20998345096161\n",
      "\t7\tlab-data\\documents\\email08\t0.9151546430082554\n"
     ]
    }
   ],
   "source": [
    "for grp in res.keys():\n",
    "    print(f\"Group {grp}\")\n",
    "    for docnum in res[grp]:\n",
    "        print(f\"\\t{docnum}\\t{mySearcherDate.stored_fields(docnum)['file_path']}\\t{sampleQueryResultsDate.score(docnum)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
